// Tempo Lexer - Self-hosted compiler
module tempo::lexer;

import core;
use core::{malloc, free};
import io;
use io::print_int;

// Token types
export enum TokenType {
    // Literals
    TOK_INTEGER,
    TOK_FLOAT,
    TOK_STRING,
    TOK_CHAR,
    TOK_IDENTIFIER,
    
    // Keywords
    TOK_FUNCTION,
    TOK_RETURN,
    TOK_IF,
    TOK_ELSE,
    TOK_WHILE,
    TOK_FOR,
    TOK_LET,
    TOK_CONST,
    TOK_STATIC,
    TOK_STRUCT,
    TOK_ENUM,
    TOK_MATCH,
    TOK_TYPE,
    TOK_IMPORT,
    TOK_MODULE,
    TOK_EXPORT,
    TOK_USE,
    TOK_AS,
    TOK_TRUE,
    TOK_FALSE,
    TOK_NULL,
    TOK_BREAK,
    TOK_CONTINUE,
    TOK_UNSAFE,
    TOK_EXTERN,
    TOK_PTR,
    TOK_ARRAY,
    TOK_VOID,
    TOK_INT8,
    TOK_INT16,
    TOK_INT32,
    TOK_INT64,
    TOK_UINT8,
    TOK_UINT16,
    TOK_UINT32,
    TOK_UINT64,
    TOK_FLOAT32,
    TOK_FLOAT64,
    TOK_BOOL,
    TOK_CHAR_TYPE,
    
    // Operators
    TOK_PLUS,
    TOK_MINUS,
    TOK_STAR,
    TOK_SLASH,
    TOK_PERCENT,
    TOK_AMPERSAND,
    TOK_PIPE,
    TOK_CARET,
    TOK_TILDE,
    TOK_LSHIFT,
    TOK_RSHIFT,
    TOK_EQ,
    TOK_NE,
    TOK_LT,
    TOK_GT,
    TOK_LE,
    TOK_GE,
    TOK_AND,
    TOK_OR,
    TOK_NOT,
    TOK_ASSIGN,
    TOK_PLUS_ASSIGN,
    TOK_MINUS_ASSIGN,
    TOK_STAR_ASSIGN,
    TOK_SLASH_ASSIGN,
    TOK_ARROW,
    TOK_DOT,
    TOK_DOUBLE_COLON,
    
    // Delimiters
    TOK_LPAREN,
    TOK_RPAREN,
    TOK_LBRACE,
    TOK_RBRACE,
    TOK_LBRACKET,
    TOK_RBRACKET,
    TOK_SEMICOLON,
    TOK_COLON,
    TOK_COMMA,
    TOK_AT,
    
    // Special
    TOK_EOF,
    TOK_ERROR,
    
    // Annotations
    TOK_WCET,
    TOK_SECURITY,
    TOK_DETERMINISTIC,
    TOK_CONSTANT_TIME,
    TOK_CACHE_PARTITION,
    TOK_TRANSACTION,
    TOK_TASK
}

// Token structure
export struct Token {
    type: TokenType,
    value: array<char, 256>,  // Fixed size for simplicity
    line: int32,
    column: int32,
    int_value: int64,
    float_value: float64
}

// Lexer state
export struct Lexer {
    input: ptr<char>,
    position: int32,
    read_position: int32,
    line: int32,
    column: int32,
    current_char: char,
    input_length: int32
}

// Create new lexer
@wcet(100)
export function lexer_new(input: ptr<char>, length: int32) -> ptr<Lexer> {
    let lexer = malloc(sizeof(Lexer)) as ptr<Lexer>;
    lexer->input = input;
    lexer->position = 0;
    lexer->read_position = 0;
    lexer->line = 1;
    lexer->column = 1;
    lexer->input_length = length;
    lexer->current_char = 0;
    
    // Read first character
    read_char(lexer);
    
    return lexer;
}

// Read next character
@wcet(50)
function read_char(lexer: ptr<Lexer>) -> void {
    if lexer->read_position >= lexer->input_length {
        lexer->current_char = 0;
    } else {
        lexer->current_char = lexer->input[lexer->read_position];
    }
    
    lexer->position = lexer->read_position;
    lexer->read_position = lexer->read_position + 1;
    
    if lexer->current_char == '\n' {
        lexer->line = lexer->line + 1;
        lexer->column = 1;
    } else {
        lexer->column = lexer->column + 1;
    }
}

// Peek next character
@wcet(20)
function peek_char(lexer: ptr<Lexer>) -> char {
    if lexer->read_position >= lexer->input_length {
        return 0;
    }
    return lexer->input[lexer->read_position];
}

// Skip whitespace
@wcet(200)
function skip_whitespace(lexer: ptr<Lexer>) -> void {
    while lexer->current_char == ' ' || 
          lexer->current_char == '\t' || 
          lexer->current_char == '\r' || 
          lexer->current_char == '\n' {
        read_char(lexer);
    }
}

// Skip comments
@wcet(500)
function skip_comments(lexer: ptr<Lexer>) -> bool {
    if lexer->current_char == '/' && peek_char(lexer) == '/' {
        // Line comment
        while lexer->current_char != '\n' && lexer->current_char != 0 {
            read_char(lexer);
        }
        return true;
    }
    
    if lexer->current_char == '/' && peek_char(lexer) == '*' {
        // Block comment
        read_char(lexer); // Skip /
        read_char(lexer); // Skip *
        
        while true {
            if lexer->current_char == 0 {
                break;
            }
            if lexer->current_char == '*' && peek_char(lexer) == '/' {
                read_char(lexer); // Skip *
                read_char(lexer); // Skip /
                break;
            }
            read_char(lexer);
        }
        return true;
    }
    
    return false;
}

// Check if character is letter
@wcet(10)
function is_letter(ch: char) -> bool {
    return (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || ch == '_';
}

// Check if character is digit
@wcet(10)
function is_digit(ch: char) -> bool {
    return ch >= '0' && ch <= '9';
}

// Check if character is hex digit
@wcet(15)
function is_hex_digit(ch: char) -> bool {
    return is_digit(ch) || (ch >= 'a' && ch <= 'f') || (ch >= 'A' && ch <= 'F');
}

// Read identifier or keyword
@wcet(500)
function read_identifier(lexer: ptr<Lexer>, token: ptr<Token>) -> void {
    let start_pos = lexer->position;
    let i = 0;
    
    while is_letter(lexer->current_char) || is_digit(lexer->current_char) {
        if i < 255 {
            token->value[i] = lexer->current_char;
            i = i + 1;
        }
        read_char(lexer);
    }
    token->value[i] = 0;
    
    // Check for keywords
    if str_equals(token->value, "function") {
        token->type = TokenType::TOK_FUNCTION;
    } else if str_equals(token->value, "return") {
        token->type = TokenType::TOK_RETURN;
    } else if str_equals(token->value, "if") {
        token->type = TokenType::TOK_IF;
    } else if str_equals(token->value, "else") {
        token->type = TokenType::TOK_ELSE;
    } else if str_equals(token->value, "while") {
        token->type = TokenType::TOK_WHILE;
    } else if str_equals(token->value, "for") {
        token->type = TokenType::TOK_FOR;
    } else if str_equals(token->value, "let") {
        token->type = TokenType::TOK_LET;
    } else if str_equals(token->value, "const") {
        token->type = TokenType::TOK_CONST;
    } else if str_equals(token->value, "static") {
        token->type = TokenType::TOK_STATIC;
    } else if str_equals(token->value, "struct") {
        token->type = TokenType::TOK_STRUCT;
    } else if str_equals(token->value, "enum") {
        token->type = TokenType::TOK_ENUM;
    } else if str_equals(token->value, "match") {
        token->type = TokenType::TOK_MATCH;
    } else if str_equals(token->value, "type") {
        token->type = TokenType::TOK_TYPE;
    } else if str_equals(token->value, "import") {
        token->type = TokenType::TOK_IMPORT;
    } else if str_equals(token->value, "module") {
        token->type = TokenType::TOK_MODULE;
    } else if str_equals(token->value, "export") {
        token->type = TokenType::TOK_EXPORT;
    } else if str_equals(token->value, "use") {
        token->type = TokenType::TOK_USE;
    } else if str_equals(token->value, "as") {
        token->type = TokenType::TOK_AS;
    } else if str_equals(token->value, "true") {
        token->type = TokenType::TOK_TRUE;
    } else if str_equals(token->value, "false") {
        token->type = TokenType::TOK_FALSE;
    } else if str_equals(token->value, "null") {
        token->type = TokenType::TOK_NULL;
    } else if str_equals(token->value, "break") {
        token->type = TokenType::TOK_BREAK;
    } else if str_equals(token->value, "continue") {
        token->type = TokenType::TOK_CONTINUE;
    } else if str_equals(token->value, "unsafe") {
        token->type = TokenType::TOK_UNSAFE;
    } else if str_equals(token->value, "extern") {
        token->type = TokenType::TOK_EXTERN;
    } else if str_equals(token->value, "ptr") {
        token->type = TokenType::TOK_PTR;
    } else if str_equals(token->value, "array") {
        token->type = TokenType::TOK_ARRAY;
    } else if str_equals(token->value, "void") {
        token->type = TokenType::TOK_VOID;
    } else if str_equals(token->value, "int8") {
        token->type = TokenType::TOK_INT8;
    } else if str_equals(token->value, "int16") {
        token->type = TokenType::TOK_INT16;
    } else if str_equals(token->value, "int32") {
        token->type = TokenType::TOK_INT32;
    } else if str_equals(token->value, "int64") {
        token->type = TokenType::TOK_INT64;
    } else if str_equals(token->value, "uint8") {
        token->type = TokenType::TOK_UINT8;
    } else if str_equals(token->value, "uint16") {
        token->type = TokenType::TOK_UINT16;
    } else if str_equals(token->value, "uint32") {
        token->type = TokenType::TOK_UINT32;
    } else if str_equals(token->value, "uint64") {
        token->type = TokenType::TOK_UINT64;
    } else if str_equals(token->value, "float32") {
        token->type = TokenType::TOK_FLOAT32;
    } else if str_equals(token->value, "float64") {
        token->type = TokenType::TOK_FLOAT64;
    } else if str_equals(token->value, "bool") {
        token->type = TokenType::TOK_BOOL;
    } else if str_equals(token->value, "char") {
        token->type = TokenType::TOK_CHAR_TYPE;
    } else {
        token->type = TokenType::TOK_IDENTIFIER;
    }
}

// Read number
@wcet(500)
function read_number(lexer: ptr<Lexer>, token: ptr<Token>) -> void {
    let i = 0;
    let is_hex = false;
    let is_float = false;
    
    // Check for hex prefix
    if lexer->current_char == '0' && (peek_char(lexer) == 'x' || peek_char(lexer) == 'X') {
        is_hex = true;
        token->value[i] = lexer->current_char;
        i = i + 1;
        read_char(lexer);
        token->value[i] = lexer->current_char;
        i = i + 1;
        read_char(lexer);
    }
    
    // Read digits
    if is_hex {
        while is_hex_digit(lexer->current_char) {
            if i < 255 {
                token->value[i] = lexer->current_char;
                i = i + 1;
            }
            read_char(lexer);
        }
    } else {
        while is_digit(lexer->current_char) {
            if i < 255 {
                token->value[i] = lexer->current_char;
                i = i + 1;
            }
            read_char(lexer);
        }
        
        // Check for decimal point
        if lexer->current_char == '.' && is_digit(peek_char(lexer)) {
            is_float = true;
            token->value[i] = lexer->current_char;
            i = i + 1;
            read_char(lexer);
            
            while is_digit(lexer->current_char) {
                if i < 255 {
                    token->value[i] = lexer->current_char;
                    i = i + 1;
                }
                read_char(lexer);
            }
        }
    }
    
    token->value[i] = 0;
    
    if is_float {
        token->type = TokenType::TOK_FLOAT;
        // TODO: Implement string to float conversion
        token->float_value = 0.0;
    } else {
        token->type = TokenType::TOK_INTEGER;
        token->int_value = str_to_int(token->value, is_hex);
    }
}

// Read string literal
@wcet(1000)
function read_string(lexer: ptr<Lexer>, token: ptr<Token>) -> void {
    let i = 0;
    read_char(lexer); // Skip opening quote
    
    while lexer->current_char != '"' && lexer->current_char != 0 {
        if lexer->current_char == '\\' {
            read_char(lexer);
            if lexer->current_char == 'n' {
                token->value[i] = '\n';
            } else if lexer->current_char == 't' {
                token->value[i] = '\t';
            } else if lexer->current_char == 'r' {
                token->value[i] = '\r';
            } else if lexer->current_char == '\\' {
                token->value[i] = '\\';
            } else if lexer->current_char == '"' {
                token->value[i] = '"';
            } else {
                token->value[i] = lexer->current_char;
            }
        } else {
            token->value[i] = lexer->current_char;
        }
        
        i = i + 1;
        if i >= 255 {
            break;
        }
        read_char(lexer);
    }
    
    token->value[i] = 0;
    token->type = TokenType::TOK_STRING;
    
    if lexer->current_char == '"' {
        read_char(lexer); // Skip closing quote
    } else {
        token->type = TokenType::TOK_ERROR;
    }
}

// Read character literal
@wcet(100)
function read_char_literal(lexer: ptr<Lexer>, token: ptr<Token>) -> void {
    read_char(lexer); // Skip opening quote
    
    if lexer->current_char == '\\' {
        read_char(lexer);
        if lexer->current_char == 'n' {
            token->value[0] = '\n';
        } else if lexer->current_char == 't' {
            token->value[0] = '\t';
        } else if lexer->current_char == 'r' {
            token->value[0] = '\r';
        } else if lexer->current_char == '\\' {
            token->value[0] = '\\';
        } else if lexer->current_char == '\'' {
            token->value[0] = '\'';
        } else {
            token->value[0] = lexer->current_char;
        }
        read_char(lexer);
    } else {
        token->value[0] = lexer->current_char;
        read_char(lexer);
    }
    
    token->value[1] = 0;
    token->type = TokenType::TOK_CHAR;
    token->int_value = token->value[0] as int64;
    
    if lexer->current_char == '\'' {
        read_char(lexer); // Skip closing quote
    } else {
        token->type = TokenType::TOK_ERROR;
    }
}

// Get next token
@wcet(2000)
export function lexer_next_token(lexer: ptr<Lexer>) -> Token {
    let token = Token {
        type: TokenType::TOK_EOF,
        value: [0; 256],
        line: lexer->line,
        column: lexer->column,
        int_value: 0,
        float_value: 0.0
    };
    
    // Skip whitespace and comments
    while true {
        skip_whitespace(lexer);
        if !skip_comments(lexer) {
            break;
        }
    }
    
    token.line = lexer->line;
    token.column = lexer->column;
    
    // Check for EOF
    if lexer->current_char == 0 {
        token.type = TokenType::TOK_EOF;
        return token;
    }
    
    // Identifiers and keywords
    if is_letter(lexer->current_char) {
        read_identifier(lexer, &token);
        return token;
    }
    
    // Numbers
    if is_digit(lexer->current_char) {
        read_number(lexer, &token);
        return token;
    }
    
    // String literals
    if lexer->current_char == '"' {
        read_string(lexer, &token);
        return token;
    }
    
    // Character literals
    if lexer->current_char == '\'' {
        read_char_literal(lexer, &token);
        return token;
    }
    
    // Operators and delimiters
    match lexer->current_char {
        '+' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_PLUS_ASSIGN;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_PLUS;
            }
        },
        '-' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_MINUS_ASSIGN;
            } else if peek_char(lexer) == '>' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_ARROW;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_MINUS;
            }
        },
        '*' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_STAR_ASSIGN;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_STAR;
            }
        },
        '/' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_SLASH_ASSIGN;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_SLASH;
            }
        },
        '%' => {
            read_char(lexer);
            token.type = TokenType::TOK_PERCENT;
        },
        '&' => {
            if peek_char(lexer) == '&' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_AND;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_AMPERSAND;
            }
        },
        '|' => {
            if peek_char(lexer) == '|' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_OR;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_PIPE;
            }
        },
        '^' => {
            read_char(lexer);
            token.type = TokenType::TOK_CARET;
        },
        '~' => {
            read_char(lexer);
            token.type = TokenType::TOK_TILDE;
        },
        '=' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_EQ;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_ASSIGN;
            }
        },
        '!' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_NE;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_NOT;
            }
        },
        '<' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_LE;
            } else if peek_char(lexer) == '<' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_LSHIFT;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_LT;
            }
        },
        '>' => {
            if peek_char(lexer) == '=' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_GE;
            } else if peek_char(lexer) == '>' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_RSHIFT;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_GT;
            }
        },
        '(' => {
            read_char(lexer);
            token.type = TokenType::TOK_LPAREN;
        },
        ')' => {
            read_char(lexer);
            token.type = TokenType::TOK_RPAREN;
        },
        '{' => {
            read_char(lexer);
            token.type = TokenType::TOK_LBRACE;
        },
        '}' => {
            read_char(lexer);
            token.type = TokenType::TOK_RBRACE;
        },
        '[' => {
            read_char(lexer);
            token.type = TokenType::TOK_LBRACKET;
        },
        ']' => {
            read_char(lexer);
            token.type = TokenType::TOK_RBRACKET;
        },
        ';' => {
            read_char(lexer);
            token.type = TokenType::TOK_SEMICOLON;
        },
        ':' => {
            if peek_char(lexer) == ':' {
                read_char(lexer);
                read_char(lexer);
                token.type = TokenType::TOK_DOUBLE_COLON;
            } else {
                read_char(lexer);
                token.type = TokenType::TOK_COLON;
            }
        },
        ',' => {
            read_char(lexer);
            token.type = TokenType::TOK_COMMA;
        },
        '.' => {
            read_char(lexer);
            token.type = TokenType::TOK_DOT;
        },
        '@' => {
            read_char(lexer);
            token.type = TokenType::TOK_AT;
            
            // Check for annotations
            if is_letter(lexer->current_char) {
                read_identifier(lexer, &token);
                if str_equals(token->value, "wcet") {
                    token.type = TokenType::TOK_WCET;
                } else if str_equals(token->value, "security") {
                    token.type = TokenType::TOK_SECURITY;
                } else if str_equals(token->value, "deterministic") {
                    token.type = TokenType::TOK_DETERMINISTIC;
                } else if str_equals(token->value, "constant_time") {
                    token.type = TokenType::TOK_CONSTANT_TIME;
                } else if str_equals(token->value, "cache_partition") {
                    token.type = TokenType::TOK_CACHE_PARTITION;
                } else if str_equals(token->value, "transaction") {
                    token.type = TokenType::TOK_TRANSACTION;
                } else if str_equals(token->value, "task") {
                    token.type = TokenType::TOK_TASK;
                }
            }
        },
        _ => {
            read_char(lexer);
            token.type = TokenType::TOK_ERROR;
        }
    }
    
    return token;
}

// Helper: String comparison
@wcet(200)
function str_equals(s1: ptr<char>, s2: ptr<char>) -> bool {
    let i = 0;
    while s1[i] != 0 && s2[i] != 0 {
        if s1[i] != s2[i] {
            return false;
        }
        i = i + 1;
    }
    return s1[i] == s2[i];
}

// Helper: String to integer conversion
@wcet(300)
function str_to_int(str: ptr<char>, is_hex: bool) -> int64 {
    let result: int64 = 0;
    let i = 0;
    
    if is_hex {
        // Skip 0x prefix
        i = 2;
        while str[i] != 0 {
            result = result * 16;
            if str[i] >= '0' && str[i] <= '9' {
                result = result + (str[i] - '0') as int64;
            } else if str[i] >= 'a' && str[i] <= 'f' {
                result = result + (str[i] - 'a' + 10) as int64;
            } else if str[i] >= 'A' && str[i] <= 'F' {
                result = result + (str[i] - 'A' + 10) as int64;
            }
            i = i + 1;
        }
    } else {
        while str[i] != 0 {
            result = result * 10 + (str[i] - '0') as int64;
            i = i + 1;
        }
    }
    
    return result;
}

// Test lexer
@wcet(10000)
export function test_lexer() -> void {
    let test_code = "function main() -> int32 {\n    let x = 42;\n    return x + 1;\n}";
    let lexer = lexer_new(test_code, 60);
    
    io::println("Lexer Test:");
    
    while true {
        let token = lexer_next_token(lexer);
        if token.type == TokenType::TOK_EOF {
            break;
        }
        
        io::print("Token: ");
        print_token_type(token.type);
        io::print(" at ");
        print_int(token.line);
        io::print(":");
        print_int(token.column);
        io::println("");
    }
    
    free(lexer as ptr<void>);
}

// Helper: Print token type
@wcet(50)
function print_token_type(type: TokenType) -> void {
    match type {
        TokenType::TOK_INTEGER => io::print("INTEGER"),
        TokenType::TOK_IDENTIFIER => io::print("IDENTIFIER"),
        TokenType::TOK_FUNCTION => io::print("FUNCTION"),
        TokenType::TOK_RETURN => io::print("RETURN"),
        TokenType::TOK_LET => io::print("LET"),
        TokenType::TOK_ASSIGN => io::print("ASSIGN"),
        TokenType::TOK_PLUS => io::print("PLUS"),
        TokenType::TOK_SEMICOLON => io::print("SEMICOLON"),
        TokenType::TOK_LPAREN => io::print("LPAREN"),
        TokenType::TOK_RPAREN => io::print("RPAREN"),
        TokenType::TOK_LBRACE => io::print("LBRACE"),
        TokenType::TOK_RBRACE => io::print("RBRACE"),
        TokenType::TOK_ARROW => io::print("ARROW"),
        TokenType::TOK_INT32 => io::print("INT32"),
        _ => io::print("OTHER")
    }
}